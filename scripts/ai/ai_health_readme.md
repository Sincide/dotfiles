# AI Health Check - Local LLM Diagnostics

Comprehensive monitoring and diagnostics for your local AI setup (Ollama, GPU acceleration, model performance).

## âœ¨ Features

- ğŸ–¥ï¸ **System Resource Monitoring** - RAM, CPU, disk space analysis
- ğŸ¯ **GPU Acceleration Detection** - NVIDIA/AMD GPU usage and VRAM monitoring
- ğŸ¤– **Ollama Service Health** - Service status, API connectivity, version info
- ğŸ“Š **Model Analysis** - Installed models, sizes, categories, benchmarking
- âš¡ **Performance Testing** - Real-world speed tests with tokens/second metrics
- ğŸ’¡ **Smart Recommendations** - Optimization tips based on your hardware
- ğŸ“± **Interactive Menus** - Easy-to-use interface for all diagnostics

## ğŸš€ Quick Start

### Installation

1. **Download the script:**
   ```fish
   curl -O https://example.com/ai-health.fish
   chmod +x ai-health.fish
   ```

2. **Run health check:**
   ```fish
   ./ai-health.fish
   ```

### Prerequisites

- **Fish Shell** - Required for running the script
- **Ollama** (optional) - For AI model diagnostics
- **nvidia-smi** (optional) - For NVIDIA GPU monitoring
- **rocm-smi** (optional) - For AMD GPU monitoring

## ğŸ“š Usage

### Interactive Mode (Recommended)
```fish
./ai-health.fish
```

This opens a comprehensive menu:
```
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚            AI Health Check                  â”‚
â”‚         Local LLM Diagnostics               â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

What would you like to check?

  1ï¸âƒ£  Full Health Check (comprehensive)
  2ï¸âƒ£  Quick Status (fast overview)
  3ï¸âƒ£  GPU Diagnostics (detailed GPU info)
  4ï¸âƒ£  Model Analysis (installed models)
  5ï¸âƒ£  Performance Benchmark (speed test)
  6ï¸âƒ£  System Resources (RAM, CPU, disk)
  7ï¸âƒ£  Ollama Service Status
  8ï¸âƒ£  Recommendations (optimization tips)

  0ï¸âƒ£  Exit
```

### Command Line Mode
```fish
# Full comprehensive check
./ai-health.fish full

# Quick status overview
./ai-health.fish quick

# GPU-specific diagnostics
./ai-health.fish gpu

# Model analysis
./ai-health.fish models

# Performance benchmark
./ai-health.fish benchmark

# System resources only
./ai-health.fish system

# Ollama service status
./ai-health.fish ollama

# Optimization recommendations
./ai-health.fish recommendations

# Show help
./ai-health.fish help
```

## ğŸ” What It Checks

### System Resources
- **RAM Usage**: Total, used, available memory with warnings for low RAM
- **CPU Information**: Model, core count, architecture details
- **Disk Space**: Available space with warnings for low storage
- **Performance Impact**: Resource usage recommendations

### GPU Acceleration
- **GPU Detection**: NVIDIA, AMD, Intel GPU identification
- **VRAM Monitoring**: Memory usage, total capacity, utilization
- **Driver Status**: CUDA, ROCm, driver version detection
- **Acceleration Testing**: Real-time GPU usage during inference

### Ollama Health
- **Service Status**: Running/stopped, memory usage, API connectivity
- **Version Information**: Ollama version, installation path
- **API Testing**: Endpoint responsiveness, connection health
- **Process Monitoring**: PID, memory consumption, startup status

### Model Analysis
- **Installed Models**: Complete list with sizes and categories
- **Model Categorization**: 
  - ğŸ”§ Code (qwen2.5-coder, codegemma)
  - ğŸ¦™ Chat (llama models)
  - ğŸ’¬ General (mistral, etc.)
- **Storage Usage**: Total disk space used by models
- **Model Metadata**: IDs, modification dates, download status

### Performance Benchmarking
- **Response Time**: End-to-end inference speed
- **Tokens Per Second**: Throughput measurements
- **Performance Rating**: Excellent/Good/Slow/Very Slow classification
- **Comparative Analysis**: Performance across different models

## ğŸ“Š Sample Output

### Quick Status
```
â”â”â” Quick Stats â”â”â”
[â„¹] System: AMD Ryzen 9 5900X, 32GB RAM
[â„¹] GPU: NVIDIA GeForce RTX 3080
[â„¹] Ollama: 5 models installed
[âœ“] Ollama: Responding normally
[â„¹] GPU VRAM: 2847 MB used
```

### GPU Information
```
â”â”â” GPU Information â”â”â”
â”Œâ”€ GPU Information
â”‚ âœ“ NVIDIA GPU detected
â”‚ GPU: NVIDIA GeForce RTX 3080
â”‚ VRAM: 2847 MB / 10240 MB used
â”‚ Utilization: 15%
â”‚ âœ“ CUDA 12.0 available
```

### Model Analysis
```
â”â”â” Installed Models â”â”â”
â”Œâ”€ Installed Models
â”‚ âœ“ qwen2.5-coder:14b (9.0 GB) - ğŸ”§ Code/Chat
â”‚ ID: 9ec8897f747e
â”‚ Modified: 7 hours ago

â”‚ âœ“ mistral:7b-instruct (4.1 GB) - ğŸ’¬ Chat
â”‚ ID: 3944fe81ec14
â”‚ Modified: About an hour ago

â”‚ Total models: 2
â”‚ Total size: 13.1 GB
```

### Performance Benchmark
```
â”â”â” Benchmarking qwen2.5-coder:14b â”â”â”
â”Œâ”€ Benchmarking qwen2.5-coder:14b
â”‚ âœ“ Response time: 2.34 seconds
â”‚ âœ“ Tokens per second: 18.5
â”‚ Response: Sure! Here are the numbers from 1 to 10: 1, 2...
â”‚ â—¦ Performance: Good
```

## ğŸ›  Advanced Features

### GPU Acceleration Testing
The script actively tests GPU usage by:
1. Measuring baseline VRAM usage
2. Running a test inference
3. Measuring VRAM increase during inference
4. Determining if GPU acceleration is actually working

### Smart Recommendations
Based on your system, it suggests:
- **RAM Upgrades**: For better large model performance
- **GPU Drivers**: Installation recommendations
- **Model Selection**: Best models for your hardware
- **Optimization Tips**: Performance tuning advice

### Model Categorization
Automatically categorizes models by purpose:
- **Coding Models**: qwen2.5-coder, codegemma, etc.
- **Chat Models**: llama, mistral, etc.
- **Specialized**: Vision, embedding, etc.

## ğŸš¨ Troubleshooting

### Common Issues

**"Ollama not installed"**
```fish
curl -fsSL https://ollama.ai/install.sh | sh
```

**"Ollama service not running"**
```fish
ollama serve
```

**"No models installed"**
```fish
ollama pull llama3.2:3b
ollama pull qwen2.5-coder:7b
```

**"GPU not detected"**
- Install NVIDIA drivers: Check your distro's instructions
- Install CUDA: `sudo apt install nvidia-cuda-toolkit`
- Verify: `nvidia-smi`

**"Poor performance"**
- Check available RAM: Close other applications
- Monitor GPU usage: Ensure GPU acceleration is working
- Try smaller models: qwen2.5-coder:7b instead of :14b

### Debug Mode
For detailed diagnostics:
```fish
# Add debug output to any command
DEBUG=1 ./ai-health.fish gpu
```

## ğŸ¯ Hardware Recommendations

### Minimum Requirements
- **RAM**: 8GB (for 7B models)
- **Storage**: 10GB free space
- **CPU**: Modern multi-core processor

### Recommended Setup
- **RAM**: 16GB+ (for 14B+ models)
- **GPU**: NVIDIA RTX 3060+ or equivalent
- **Storage**: SSD with 50GB+ free space
- **CPU**: Recent AMD/Intel with 8+ cores

### Optimal Setup
- **RAM**: 32GB+ (for multiple large models)
- **GPU**: NVIDIA RTX 4070+ with 12GB+ VRAM
- **Storage**: NVMe SSD with 100GB+ free space
- **CPU**: High-end AMD Ryzen or Intel Core

## ğŸ“ˆ Performance Expectations

### Model Size vs Performance
- **1B-3B models**: 50+ tokens/sec (CPU), 100+ tokens/sec (GPU)
- **7B models**: 10-30 tokens/sec (CPU), 50+ tokens/sec (GPU)
- **14B+ models**: 5-15 tokens/sec (CPU), 20+ tokens/sec (GPU)

### GPU vs CPU Performance
- **GPU Acceleration**: 3-10x faster than CPU
- **VRAM Requirements**: Model size + 2-4GB overhead
- **Memory Bandwidth**: Critical for large models

## ğŸ¤ Contributing

Contributions welcome! Areas for improvement:
- Additional GPU vendor support (Intel Arc, etc.)
- More model benchmarks
- Advanced performance profiling
- Container support (Docker, Podman)

### Development
Key functions:
- `check_gpu()` - GPU detection and monitoring
- `check_ollama_service()` - Ollama health checks
- `benchmark_model()` - Performance testing
- `health_recommendations()` - Smart suggestions

## ğŸ“„ License

This script is provided as-is for personal and educational use.

---

**Keep your AI healthy! ğŸ¤–ğŸ’ª**